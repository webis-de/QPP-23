{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87921a3b-94d9-4759-be40-acc2b0f3035d",
   "metadata": {},
   "source": [
    "# Produce Final Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e944cbbe-9d00-4f72-ad91-c48b13bd27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = '/mnt/ceph/storage/data-in-progress/data-research/web-search/false-memories/reddit-tomt/tomt-dataset-03-12-2022/'\n",
    "\n",
    "def load_json_data(file_name):\n",
    "    with gzip.open(DATA_DIR + file_name, 'r') as f:\n",
    "        for l in tqdm(f):\n",
    "            try:\n",
    "                yield json.loads(l)\n",
    "            except:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cc98f8-71d8-43f4-9aa6-099d72f343db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16627156it [05:07, 54090.54it/s]\n"
     ]
    }
   ],
   "source": [
    "parent_id_to_comments = {}\n",
    "\n",
    "for i in load_json_data('reddit-tomt-comments.jsonl.gz'):\n",
    "    if i['parent_id'] not in parent_id_to_comments:\n",
    "        parent_id_to_comments[i['parent_id']] = []\n",
    "    \n",
    "    parent_id_to_comments[i['parent_id']] += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561913f6-a15f-4790-96a6-27e3a91d24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(doc, depth):\n",
    "    ret = []\n",
    "\n",
    "    if depth > 10:\n",
    "        return []\n",
    "    \n",
    "    for parent_id in ['id', 'name']:\n",
    "        if doc and parent_id in doc and len(ret) < 100:\n",
    "            for prefix in ['', 't1_', 't2_', 't3_', 't4_']:\n",
    "                ret += parent_id_to_comments.get(prefix + doc[parent_id], [])\n",
    "\n",
    "    ret_without_duplicates = []\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for i in ret:\n",
    "        if i['id'] in existing_ids:\n",
    "            continue\n",
    "        existing_ids.add(i['id'])\n",
    "        ret_without_duplicates += [i]\n",
    "        \n",
    "    return ret_without_duplicates\n",
    "\n",
    "def enrich_tomt_with_comments(doc, depth):\n",
    "    doc = deepcopy(doc)\n",
    "    \n",
    "    comments = get_comments(doc, depth)\n",
    "    comments = [enrich_tomt_with_comments(i, depth+1) for i in comments]\n",
    "    doc['comments'] = comments\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ccd059-899f-4cad-8bcd-436ea343c4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2558850it [14:10, 3007.38it/s]\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(DATA_DIR + 'reddit-tomt-submissions-with-comments.jsonl.gz', 'w') as f:\n",
    "    for tomt in load_json_data('reddit-tomt-submissions.jsonl.gz'):\n",
    "        tomt = enrich_tomt_with_comments(tomt, 0)\n",
    "        f.write((json.dumps(tomt) + '\\n').encode('utf8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7aa1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
